#!/bin/bash
set -e
OUTPUT=/dev/null

################################################################################
# TODO: Configuration Explanation
#

# Convert settings into bash arrays
controllerNames=(${CONTROLLER_NAMES//,/ })
controllerSshIps=(${CONTROLLER_SSH_IPS//,/ })
controllerPrivateIps=(${CONTROLLER_PRIVATE_IPS//,/ })
etcdNames=(${ETCD_NAMES//,/ })
etcdSshIps=(${ETCD_SSH_IPS//,/ })
etcdPrivateIps=(${ETCD_PRIVATE_IPS//,/ })
nodeNames=(${NODE_NAMES//,/ })
nodeSshIps=(${NODE_SSH_IPS//,/ })
nodePrivateIps=(${NODE_PRIVATE_IPS//,/ })
for idx in ${!etcdNames[@]}; do
  initialCluster+=",${etcdNames[$idx]}=https://${etcdPrivateIps[$idx]}:2380"
  etcdHosts+=",https://${etcdPrivateIps[$idx]}:2379"
done

# User to SSH as
sshUser=${SSH_USER?'not defined.'}

# Name of cluster.
clusterName=${CLUSTER_NAME?'not defined.'}

# Network to use for Pods
podCidr=${POD_CIDR?'not defined.'}

# Network to use for Services.
serviceCidr=${SERVICE_CIDR?'not defined.'}

kubernetesServiceIp=${KUBERNETES_SERVICE_IP?'not defined.'}

# Public IP of first controller VM.
apiserver=${controllerSshIps[0]}

# All etcd host/ips for bootstrapping etcd, as comma delimited string.
# Used as command line argument for starting etcd.
initialCluster=${initialCluster:1}

# All etcd hosts, for kube-apiserver, as comma delimited string.
# Used as command line argument for starting etcd.
etcdHosts=${etcdHosts:1}

# Global variables for logging.
logPrefix=""
clusterPath=cluster/$clusterName
manifestPath=cluster/$clusterName/manifests
pkiPath=cluster/$clusterName/pki
configPath=/etc/kubernetes

################################################################################
# Code explanation.
#
function main {
  pki
  etcd
  control-plane
  nodes
  # TODO: figure out why system:kube-proxy doesn't work as a user
  kubectl create clusterrolebinding proxy-binding --clusterrole=cluster-admin --user=kube-proxy
  dns
}

function pki {
  mkdir -p $pkiPath

  # Root CA for signing kubernetes component tls certs.
  init-ca cluster-ca /CN=$clusterName

  # Root CA for signing etcd components tls certs.
  init-ca etcd-ca /CN=etcd

  # Allow operator to auth with apiserver using kubectl
  private-key root
  signed-cert cluster-ca root /CN=root/O=system:masters

  # TODO: get TLS bootstrapping going
  # https://kubernetes.io/docs/admin/bootstrap-tokens/
  private-key service-account
  public-key service-account

  # Prepare PKI package for each etcd host.
  for idx in ${!etcdNames[@]}; do
    local name=${etcdNames[$idx]}
    local privateIp=${etcdPrivateIps[$idx]}
    mkdir -p $pkiPath/$name
    private-key $name/etcd
    private-key $name/etcd-peer
    signed-cert etcd-ca $name/etcd /CN=etcd "IP:$privateIp,DNS:$name"
    signed-cert etcd-ca $name/etcd-peer /CN=etcd-peer "IP:$privateIp,DNS:$name"
    cp $pkiPath/etcd*pem $pkiPath/$name
  done

  # Prepare PKI package for each controller host.
  for idx in ${!controllerNames[@]}; do
    local name=${controllerNames[$idx]}
    local sshIp=${controllerSshIps[$idx]}
    local privateIp=${controllerPrivateIps[$idx]}
    # TODO: remove sshIp when load balancer is in place
    mkdir -p $pkiPath/$name
    private-key $name/apiserver
    private-key $name/apiserver-to-kubelet
    signed-cert cluster-ca $name/apiserver /CN=kube-apiserver "IP:$kubernetesServiceIp,IP:$sshIp,IP:$privateIp,DNS:$name,DNS:kubernetes,DNS:kubernetes.default,DNS:kubernetes.default.svc,DNS:kubernetes.default.svc.cluster,DNS:kubernetes.default.svc.cluster.local"
    signed-cert cluster-ca $name/apiserver-to-kubelet /CN=kube-apiserver-client/O=system:masters
    cp $pkiPath/service-account*pem $pkiPath/$name
    cp $pkiPath/cluster*pem $pkiPath/$name
    cp $pkiPath/etcd*pem $pkiPath/$name
  done

  # Prepare PKI package for each worker node.
  for idx in ${!nodeNames[@]}; do
    local name=${nodeNames[$idx]}
    local privateIp=${nodePrivateIps[$idx]}
    mkdir -p $pkiPath/$name
    private-key $name/kubelet
    private-key $name/kube-router
    signed-cert cluster-ca $name/kubelet /CN=system:node:$name/O=system:nodes "IP:$privateIp,DNS:$name"
    signed-cert cluster-ca $name/kube-router /CN=kube-proxy
    cp $pkiPath/cluster-ca.pem $pkiPath/$name
  done

  # Generate config for local user to access apiserver with kubectl.
  kubectl-root-access $clusterName $apiserver
}

function etcd {
  for idx in ${!etcdNames[@]}; do
    etcd-host ${etcdNames[$idx]} ${etcdSshIps[$idx]} ${etcdPrivateIps[$idx]} $initialCluster
  done
}

function control-plane {
  for idx in ${!controllerNames[@]}; do
    controller-host $clusterName ${controllerNames[$idx]} ${controllerSshIps[$idx]} ${controllerPrivateIps[$idx]} $etcdHosts $podCidr $serviceCidr
  done
}

function dns {
  mkdir -p $manifestPath
  curl -s https://raw.githubusercontent.com/kubernetes/kubernetes/c7d693443371406842364ed74aa2ea34bfb9fcad/cluster/addons/dns/kube-dns.yaml.sed | sed -e "s/\\\$DNS_DOMAIN/cluster.local/g" | sed -e "s/\\\$DNS_SERVER_IP/${dnsServiceIp}/g" > $manifestPath/dns.yml
}

function nodes {
  for idx in ${!nodeNames[@]}; do
    node $clusterName ${nodeNames[$idx]} ${nodeSshIps[$idx]} ${nodePrivateIps[$idx]} $podCidr
  done
}

function kubectl-root-access {
  local clusterName=$1
  local apiserver=$2
  {
    kubectl config set-cluster $clusterName \
      --certificate-authority=$pkiPath/cluster-ca.pem \
      --embed-certs=true \
      --server=https://$apiserver:6443

    kubectl config set-credentials $clusterName-root \
      --client-certificate=$pkiPath/root-cert.pem \
      --client-key=$pkiPath/root-private-key.pem \
      --embed-certs=true

    kubectl config set-context $clusterName \
      --cluster=$clusterName \
      --user=$clusterName-root

    kubectl config use-context $clusterName
  } &> $OUTPUT
}

function etcd-host {
  local name=$1
  local sshIp=$2
  local privateIp=$3
  local initialCluster=$4

  logPrefix="[etcd] ($sshIp)"
  echo "$logPrefix Bootstrapping $name."
  cmd $sshIp "sudo mkdir -p $configPath"
  set-hostname $sshIp $name
  copy-files $sshIp "$pkiPath/$name/*" ${configPath}
  install-etcd $sshIp "3.2.10"
  install-service $sshIp etcd "$(etcd-exec $name $privateIp $initialCluster)"
  echo "$logPrefix Done."
}

function controller-host {
  local clusterName=$1
  local name=$2
  local sshIp=$3
  local privateIp=$4
  local etcdHosts=$5
  local podCidr=$6
  local serviceCidr=$7

  logPrefix="[controller] ($sshIp)"
  echo "$logPrefix Bootstrapping $name."
  cmd $sshIp "sudo mkdir -p $configPath"
  set-hostname $sshIp $name
  copy-files $sshIp "$pkiPath/$name/*" ${configPath}
  install-k8s $sshIp kube-apiserver 1.8.4
  install-service $sshIp kube-apiserver "$(apiserver-exec $privateIp $etcdHosts $serviceCidr)"
  install-k8s $sshIp kube-controller-manager 1.8.4
  install-service $sshIp kube-controller-manager "$(controller-manager-exec $clusterName $podCidr $serviceCidr)"
  install-k8s $sshIp kube-scheduler 1.8.4
  install-service $sshIp kube-scheduler "$(scheduler-exec)"
  install-k8s $sshIp kubectl 1.8.4
  echo "$logPrefix Done."
}

function node {
  local clusterName=$1
  local name=$2
  local sshIp=$3
  local privateIp=$4
  local podCidr=$5

  logPrefix="[node] ($sshIp)"
  echo "$logPrefix Bootstrapping $name."
  cmd $sshIp "sudo mkdir -p $configPath"
  set-hostname $sshIp $name
  copy-files $sshIp "$pkiPath/$name/*" ${configPath}
  cmd $sshIp <<EOF
sudo apt-get update
sudo apt-get install docker.io -y
EOF
  # Configure networking.
  install-socat $sshIp
  install-cni-plugins $sshIp 0.6.0
  copy-var $sshIp /etc/cni/net.d/10-kuberouter.conf "$(cni-bridge $podCidr)"
  copy-var $sshIp /etc/cni/net.d/99-loopback.conf "$(cni-loopback)"
  # Configure kube-router
  copy-var $sshIp ${configPath}/kube-router.kubeconfig "$(kubeconfig $clusterName kube-router kube-router $apiserver)"
  install-kube-router $sshIp 0.0.19
  install-service $sshIp kube-router "$(kube-router-exec $privateIp $podCidr)"
  # Install kubectl.
  install-k8s $sshIp kubectl 1.8.4
  # Install and configure kubelet.
  install-k8s $sshIp kubelet 1.8.4
  copy-var $sshIp ${configPath}/kubelet.kubeconfig "$(kubeconfig $clusterName kubelet system:node:$name $apiserver)"
  install-service $sshIp kubelet "$(kubelet-exec $privateIp $podCidr)"
  echo "$logPrefix Done."
}

function install-etcd {
  local sshIp=$1
  local version=$2
  local baseUrl="https://github.com/coreos/etcd/releases/download/v$version"
  local folder="etcd-v$version-linux-amd64"
  local file="$folder.tar.gz"
  echo "$logPrefix Downloading and installing etcd v$version."
  cmd $sshIp <<EOF
sudo mkdir -p /var/lib/etcd
wget -q $baseUrl/$file
tar xf $file
sudo mkdir -p /usr/local/bin
sudo mv $folder/etcd* /usr/local/bin
EOF
}

function install-k8s {
  local sshIp=$1
  local name=$2
  local version=$3
  local baseUrl="https://storage.googleapis.com/kubernetes-release/release/v$version"
  echo "$logPrefix Downloading and installing $name v$version."
  cmd $sshIp <<EOF
wget -q $baseUrl/bin/linux/amd64/$name
chmod +x $name
sudo mkdir -p /usr/local/bin
sudo mv $name /usr/local/bin
EOF
}

function install-socat {
  local sshIp=$1
  cmd $sshIp <<EOF
sudo apt-get update -qq
sudo apt-get install socat -yqq
EOF
}

function install-kube-router {
  local sshIp=$1
  local version=$2
  local baseUrl="https://github.com/cloudnativelabs/kube-router/releases/download/v$version"
  local file="kube-router_${version}_linux_amd64.tar.gz"
  echo "$logPrefix Downloading and installing kube-router v$version."
  cmd $sshIp <<EOF
wget -q $baseUrl/$file
tar xf $file
sudo mkdir -p /usr/local/bin
sudo mv kube-router /usr/local/bin
sudo apt-get update && sudo apt-get install ipset -y
EOF
}

function install-cni-plugins {
  local sshIp=$1
  local version=$2
  local baseUrl="https://github.com/containernetworking/plugins/releases/download/v$version"
  local file="cni-plugins-amd64-v$version.tgz"
  echo "$logPrefix Downloading and installing cni plugins v$version."
  cmd $sshIp <<EOF
wget -q $baseUrl/$file
sudo mkdir -p /opt/cni/bin /etc/cni/net.d
sudo tar xf $file --directory /opt/cni/bin
EOF
}

function cni-bridge {
  local podCidr=$1
  cat <<EOF
{
  "name": "kubernetes",
  "type": "bridge",
  "bridge": "kube-bridge",
  "ipam": {
    "subnet": "$podCidr",
    "type": "host-local"
  },
  "isDefaultGateway": true
}
EOF
}

function cni-loopback {
  cat <<EOF
{
  "cniVersion": "0.3.1",
  "type": "loopback"
}
EOF
}

function etcd-exec {
  local name=$1
  local privateIp=$2
  local initialCluster=$3
  cat <<EOF
/usr/local/bin/etcd \\
  --name ${name} \\
  --cert-file=${configPath}/etcd-cert.pem \\
  --key-file=${configPath}/etcd-private-key.pem \\
  --peer-cert-file=${configPath}/etcd-peer-cert.pem \\
  --peer-key-file=${configPath}/etcd-peer-private-key.pem \\
  --trusted-ca-file=${configPath}/etcd-ca.pem \\
  --peer-trusted-ca-file=${configPath}/etcd-ca.pem \\
  --peer-client-cert-auth \\
  --client-cert-auth \\
  --initial-advertise-peer-urls https://${privateIp}:2380 \\
  --listen-peer-urls https://${privateIp}:2380 \\
  --listen-client-urls https://${privateIp}:2379,http://127.0.0.1:2379 \\
  --advertise-client-urls https://${privateIp}:2379 \\
  --initial-cluster-token etcd-cluster-0 \\
  --initial-cluster ${initialCluster} \\
  --initial-cluster-state new \\
  --data-dir=/var/lib/etcd
EOF
}

function apiserver-exec {
  local privateIp=$1
  local etcdHosts=$2
  local serviceCidr=$3
  cat <<EOF
/usr/local/bin/kube-apiserver \\
  --admission-control=Initializers,NamespaceLifecycle,NodeRestriction,LimitRanger,ServiceAccount,DefaultStorageClass,ResourceQuota \\
  --advertise-address=${privateIp} \\
  --allow-privileged=true \\
  --apiserver-count=3 \\
  --audit-log-maxage=30 \\
  --audit-log-maxbackup=3 \\
  --audit-log-maxsize=100 \\
  --audit-log-path=/var/log/audit.log \\
  --authorization-mode=Node,RBAC \\
  --bind-address=0.0.0.0 \\
  --client-ca-file=${configPath}/cluster-ca.pem \\
  --etcd-cafile=${configPath}/etcd-ca.pem \\
  --etcd-certfile=${configPath}/etcd-cert.pem \\
  --etcd-keyfile=${configPath}/etcd-private-key.pem \\
  --etcd-servers=${etcdHosts} \\
  --event-ttl=1h \\
  --insecure-bind-address=127.0.0.1 \\
  --kubelet-preferred-address-types=InternalIP,Hostname,ExternalIP \\
  --kubelet-certificate-authority=${configPath}/cluster-ca.pem \\
  --kubelet-client-certificate=${configPath}/apiserver-to-kubelet-cert.pem \\
  --kubelet-client-key=${configPath}/apiserver-to-kubelet-private-key.pem \\
  --kubelet-https=true \\
  --runtime-config=api/all \\
  --service-account-key-file=${configPath}/service-account-public-key.pem \\
  --service-cluster-ip-range=${serviceCidr} \\
  --service-node-port-range=30000-32767 \\
  --tls-ca-file=${configPath}/cluster-ca.pem \\
  --tls-cert-file=${configPath}/apiserver-cert.pem \\
  --tls-private-key-file=${configPath}/apiserver-private-key.pem \\
  --v=2
EOF
}

function controller-manager-exec {
  name=$1
  podCidr=$2
  serviceCidr=$3
  cat <<EOF
/usr/local/bin/kube-controller-manager \\
  --address=0.0.0.0 \\
  --allocate-node-cidrs=true \\
  --cluster-cidr=${podCidr} \\
  --cluster-name=${name} \\
  --cluster-signing-cert-file=${configPath}/cluster-ca.pem \\
  --cluster-signing-key-file=${configPath}/cluster-ca-private-key.pem \\
  --leader-elect=true \\
  --master=http://127.0.0.1:8080 \\
  --root-ca-file=${configPath}/cluster-ca.pem \\
  --service-account-private-key-file=${configPath}/service-account-private-key.pem \\
  --service-cluster-ip-range=${serviceCidr} \\
  --v=2
EOF
}

function scheduler-exec {
  cat <<EOF
/usr/local/bin/kube-scheduler \\
  --leader-elect=true \\
  --master=http://127.0.0.1:8080 \\
  --v=2
EOF
}

function kubelet-exec {
  local privateIp=$1
  local podCidr=$2
  cat <<EOF
/usr/local/bin/kubelet \\
  --node-ip=${privateIp} \\
  --allow-privileged=true \\
  --anonymous-auth=false \\
  --authorization-mode=Webhook \\
  --client-ca-file=${configPath}/cluster-ca.pem \\
  --cluster-dns=10.32.0.10 \\
  --cluster-domain=cluster.local \\
  --image-pull-progress-deadline=2m \\
  --kubeconfig=${configPath}/kubelet.kubeconfig \\
  --network-plugin=cni \\
  --cni-bin-dir=/opt/cni/bin \\
  --cni-conf-dir=/etc/cni/net.d \\
  --pod-cidr=${podCidr} \\
  --register-node=true \\
  --runtime-request-timeout=15m \\
  --tls-cert-file=${configPath}/kubelet-cert.pem \\
  --tls-private-key-file=${configPath}/kubelet-private-key.pem \\
  --v=2
EOF
}

function kubeconfig {
  clusterName=$1
  name=$2
  user=$3
  apiserver=$4
  cat <<EOF
apiVersion: v1
kind: Config
current-context: default
clusters:
  - cluster:
      certificate-authority: ${configPath}/cluster-ca.pem
      server: https://${apiserver}:6443
    name: ${clusterName}
contexts:
  - context:
      cluster: ${clusterName}
      user: ${user}
    name: default
users:
  - name: ${user}
    user:
      client-certificate: ${configPath}/${name}-cert.pem
      client-key: ${configPath}/${name}-private-key.pem
EOF
}

function kube-router-exec {
  local privateip=$1
  local podCidr=$2
  cat <<EOF
/usr/local/bin/kube-router \\
  --run-router=true \\
  --run-firewall=true \\
  --run-service-proxy=true \\
  --kubeconfig=${configPath}/kube-router.kubeconfig \\
  --cluster-cidr=${podCidr}
EOF
}

function install-service {
  local sshIp=$1
  local name=$2
  local start=$3
  local after=$4
  local requires=$5

  echo "$logPrefix Installing, enabling and starting ${name} service."
  copy-var $sshIp "/etc/systemd/system/$name.service" "$(cat <<EOF
[Unit]
Description=${name}
After=${after}
Requires=${requires}

[Service]
ExecStart=${start}
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
EOF
)"
  cmd $sshIp <<EOF
sudo systemctl daemon-reload
sudo systemctl enable $name
sudo systemctl restart $name
EOF
}

function set-hostname {
  # Ensure hostname is set and loopback entry is present for hosts that need
  # it. The current use-case for this is AWS support.
  sshIp=$1
  name=$2
  cmd $sshIp <<EOF
sudo grep -q -F "$name" /etc/hosts || (echo "127.0.0.1 $name" | sudo tee -a /etc/hosts)
sudo hostname $name
EOF
}

function init-ca {
  local name=$1
  local subj=$2
  {
    openssl ecparam -name secp521r1 -genkey -noout -out $pkiPath/$name-private-key.pem
    openssl req -x509 -new -sha256 -nodes -days 3650 \
      -key $pkiPath/$name-private-key.pem \
      -out $pkiPath/$name.pem \
      -subj "$subj" \
      -extensions ext \
      -config <(echo "
[req]
distinguished_name = default
[default]
[ext]
basicConstraints = critical, CA:TRUE
keyUsage = critical, digitalSignature, keyEncipherment, keyCertSign
")
} &> $OUTPUT
}

function private-key {
  local name=$1
  {
    openssl ecparam -name secp521r1 -genkey -noout -out $pkiPath/$name-private-key.pem
  } &> $OUTPUT
}

function public-key {
  local name=$1
  {
    openssl ec -in $pkiPath/$name-private-key.pem -outform PEM -pubout -out $pkiPath/$name-public-key.pem
  } &> $OUTPUT
}

function signed-cert {
  local ca=$1
  local name=$2
  local subj=$3
  local san=$4
  {
    openssl req -new -sha256 -key $pkiPath/$name-private-key.pem -subj "$subj" | \
      openssl x509 -req -sha256 \
        -CA $pkiPath/$ca.pem \
        -CAkey $pkiPath/$ca-private-key.pem \
        -CAcreateserial \
        -out $pkiPath/$name-cert.pem \
        -days 3650 \
        -extensions ext \
        -extfile <(echo "
[req]
distinguished_name = default
[default]
[ext]
basicConstraints = critical, CA:FALSE
keyUsage = critical, digitalSignature, keyEncipherment
extendedKeyUsage = serverAuth, clientAuth
$(if [ -n "$san" ]; then echo "subjectAltName = $san" ; fi)
")
} &> $OUTPUT
}

function copy-var {
  local host=$1
  local dest=$2
  local data=$3
  echo "$data" | ssh -q \
    -o StrictHostKeyChecking=no \
    -o UserKnownHostsFile=/dev/null \
    $sshUser@$host "sudo tee $dest > /dev/null"
}

function copy-files {
  local host=$1
  local glob=$2
  local dest=$3
  {
    rsync -av \
      -e "ssh -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null" \
      --rsync-path="sudo rsync" $glob $sshUser@$host:$dest
  } &> $OUTPUT
}

function cmd {
  local host=$1
  local cmd=$2
  if [ -z "$cmd" ]; then
    cmd=`cat`
  fi
  {
    ssh -q \
      -o StrictHostKeyChecking=no \
      -o UserKnownHostsFile=/dev/null \
      $sshUser@$host "$cmd"
  } &> $OUTPUT
}

main "$@"
